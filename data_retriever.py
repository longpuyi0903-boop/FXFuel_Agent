# data_retriever.py - æ•°æ®é‡‡é›†æ¨¡å—ï¼ˆä¿®å¤ç‰ˆ v2ï¼‰

import os
import ssl
import json
import time
import re
from datetime import datetime
from typing import Dict, Any, Optional, Callable
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from dotenv import load_dotenv

load_dotenv()

# P0-2: å¯¼å…¥è¶…æ—¶é…ç½®; P1: å¯¼å…¥ç¼“å­˜ TTL é…ç½®
try:
    from config import TIMEOUT_CONFIG, CACHE_TTL
except ImportError:
    # å¦‚æœ config.py æœªæ›´æ–°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    TIMEOUT_CONFIG = {
        "default": (10, 30),
        "akshare": (10, 20),
        "fred": (10, 30),
        "perplexity": (30, 90),
        "yahoo": (10, 15),
        "hkma": (10, 20),
    }
    CACHE_TTL = {
        "cny_mid": 3600,
        "cny_spot": 60,
        "hkd": 60,
        "fred": 300,
        "global_fx": 60,
        "news": 600,
    }

# SSLä¿®å¤
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def create_retry_session(retries=3, backoff_factor=1):
    session = requests.Session()
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

RETRY_SESSION = create_retry_session()


# ============================================================================
# P1: ç®€å•ç¼“å­˜æ¨¡å—
# ============================================================================

import time as _time
from typing import TypeVar

_cache: Dict[str, Any] = {}
_cache_time: Dict[str, float] = {}

T = TypeVar('T')

def get_with_cache(key: str, fetch_func: Callable[[], T], ttl_seconds: int) -> T:
    """
    å¸¦ TTL çš„ç®€å•ç¼“å­˜
    
    Args:
        key: ç¼“å­˜é”®å
        fetch_func: è·å–æ•°æ®çš„å‡½æ•°
        ttl_seconds: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        
    Returns:
        ç¼“å­˜çš„æ•°æ®æˆ–æ–°è·å–çš„æ•°æ®
        
    TTL æ¨èå€¼ï¼ˆå‚è€ƒ config.CACHE_TTLï¼‰:
    - ä¸­é—´ä»· (cny_mid): 3600ç§’ï¼ˆ9:15å‘å¸ƒåæ•´å¤©ä¸å˜ï¼‰
    - å®æ—¶æ±‡ç‡ (cny_spot): 60ç§’
    - FRED æ•°æ®: 300ç§’
    - æ–°é—»: 600ç§’
    """
    now = _time.time()
    if key in _cache and (now - _cache_time.get(key, 0)) < ttl_seconds:
        return _cache[key]
    
    result = fetch_func()
    _cache[key] = result
    _cache_time[key] = now
    return result


def clear_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆç”¨äºå¼ºåˆ¶åˆ·æ–°ï¼‰"""
    global _cache, _cache_time
    _cache = {}
    _cache_time = {}


class DataContext:
    def __init__(self):
        self.report_date = datetime.now().strftime("%Y-%m-%d")
        self.snapshot = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.cny = {}
        self.hkd = {}
        self.global_fx = {}
        self.macro = {}
        self.news = []  # çŸ­æ ‡é¢˜åˆ—è¡¨ï¼ˆç”¨äºé¡µé¢å±•ç¤ºï¼‰
        self.news_detail = []  # è¯¦ç»†æ‘˜è¦åˆ—è¡¨ï¼ˆç”¨äºLLMç”ŸæˆæŠ¥å‘Šï¼‰
        self.news_sources = []
        self.data_sources = {}
        self.errors = []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "report_date": self.report_date,
            "snapshot": self.snapshot,
            "cny": self.cny,
            "hkd": self.hkd,
            "global_fx": self.global_fx,
            "macro": self.macro,
            "news": self.news,
            "news_detail": self.news_detail,
            "news_sources": self.news_sources,
            "data_sources": self.data_sources,
            "errors": self.errors,
            "data_points": self._count_data_points()
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
    
    def _count_data_points(self) -> int:
        count = 0
        for section in [self.cny, self.hkd, self.global_fx, self.macro]:
            count += len([v for v in section.values() if v is not None])
        count += len(self.news)
        return count


ProgressCallback = Callable[[int, int, str], None]


def fetch_cny_data(ctx: DataContext) -> str:
    """è·å–äººæ°‘å¸æ•°æ®"""
    try:
        import akshare as ak
        
        try:
            mid_df = None
            for attempt in range(3):
                try:
                    mid_df = ak.currency_boc_safe()
                    if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                usd_col = mid_df['ç¾å…ƒ'].astype(float) / 100
                ctx.cny["usdcny_mid"] = round(float(usd_col.iloc[-1]), 4)
                ctx.cny["usdcny_mid_date"] = str(mid_df['æ—¥æœŸ'].iloc[-1])
                ctx.data_sources["usdcny_mid"] = "å›½å®¶å¤–æ±‡ç®¡ç†å±€"
                
                recent = usd_col.tail(5)
                ctx.cny["usdcny_mid_range"] = f"{round(recent.min(), 4)} - {round(recent.max(), 4)}"
                ctx.cny["usdcny_mid_high"] = round(recent.max(), 4)
                ctx.cny["usdcny_mid_low"] = round(recent.min(), 4)
            else:
                # API å¤±è´¥æˆ–æ•°æ®æ— æ•ˆï¼Œæ˜¾å¼è®¾ç½® None
                ctx.cny["usdcny_mid"] = None
        except Exception as e:
            ctx.errors.append(f"äººæ°‘å¸ä¸­é—´ä»·: {str(e)[:80]}")
            ctx.cny["usdcny_mid"] = None
        
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if fx_df is not None and not fx_df.empty:
                cnh_row = fx_df[fx_df['ä»£ç '].str.contains('USDCNH', case=False, na=False)]
                if not cnh_row.empty:
                    ctx.cny["usdcnh_spot"] = float(cnh_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.data_sources["usdcnh"] = "ä¸œæ–¹è´¢å¯Œ"
                else:
                    # æœªæ‰¾åˆ° USDCNH æ•°æ®
                    ctx.cny["usdcnh_spot"] = None
                # è®¡ç®—ä»·å·®ï¼ˆå¦‚æœä¸¤ä¸ªå€¼éƒ½å­˜åœ¨ï¼‰
                if ctx.cny.get("usdcny_mid") is not None and ctx.cny.get("usdcnh_spot") is not None:
                    ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
            else:
                # API å¤±è´¥æˆ–æ•°æ®æ— æ•ˆï¼Œæ˜¾å¼è®¾ç½® None
                ctx.cny["usdcnh_spot"] = None
        except Exception as e:
            ctx.errors.append(f"ç¦»å²¸æ±‡ç‡: {str(e)[:80]}")
            ctx.cny["usdcnh_spot"] = None
        
        parts = []
        if ctx.cny.get("usdcny_mid"):
            parts.append(f"ä¸­é—´ä»·:{ctx.cny['usdcny_mid']}")
        if ctx.cny.get("usdcnh_spot"):
            parts.append(f"CNH:{ctx.cny['usdcnh_spot']}")
        
        return f"âœ… äººæ°‘å¸: {', '.join(parts)}" if parts else "âš ï¸ äººæ°‘å¸æ•°æ®éƒ¨åˆ†ç¼ºå¤±"
        
    except ImportError:
        ctx.errors.append("AKShare æœªå®‰è£…")
        return "âŒ AKShare æœªå®‰è£…"
    except Exception as e:
        ctx.errors.append(f"äººæ°‘å¸æ•°æ®: {str(e)[:80]}")
        return "âŒ äººæ°‘å¸æ•°æ®è·å–å¤±è´¥"


def fetch_hkd_data(ctx: DataContext) -> str:
    """è·å–æ¸¯å…ƒæ•°æ®"""
    hkd_found = False
    
    try:
        import akshare as ak
        
        # æ–¹æ³•1: ä¸œæ–¹è´¢å¯Œå¤–æ±‡è¡Œæƒ…
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except Exception as e:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
                    else:
                        ctx.errors.append(f"ä¸œæ–¹è´¢å¯ŒAPI: {str(e)[:40]}")
            
            if fx_df is not None and not fx_df.empty:
                hkd_row = fx_df[fx_df['ä»£ç '].str.contains('USDHKD', case=False, na=False)]
                if not hkd_row.empty:
                    usdhkd = float(hkd_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.hkd["usdhkd"] = usdhkd
                    ctx.data_sources["usdhkd"] = "ä¸œæ–¹è´¢å¯Œ"
                    hkd_found = True
                    
                    if usdhkd <= 7.77:
                        ctx.hkd["lers_position"] = "å¼ºæ–¹åŒºé—´ï¼ˆæ¥è¿‘7.75å¼ºæ–¹ä¿è¯ï¼‰"
                    elif usdhkd >= 7.83:
                        ctx.hkd["lers_position"] = "å¼±æ–¹åŒºé—´ï¼ˆæ¥è¿‘7.85å¼±æ–¹ä¿è¯ï¼‰"
                    else:
                        ctx.hkd["lers_position"] = "ä¸­é—´åŒºé—´"
        except Exception as e:
            ctx.errors.append(f"æ¸¯å…ƒ(ä¸œæ–¹è´¢å¯Œ): {str(e)[:50]}")
        
        # æ–¹æ³•2: Yahoo Finance å¤‡é€‰
        if not hkd_found:
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                resp = requests.get(
                    "https://query1.finance.yahoo.com/v8/finance/chart/HKDUSD=X?interval=1d&range=1d",
                    headers=headers,
                    timeout=TIMEOUT_CONFIG["yahoo"]
                )
                if resp.status_code == 200:
                    data = resp.json()
                    if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                        meta = data['chart']['result'][0].get('meta', {})
                        hkdusd = meta.get('regularMarketPrice') or meta.get('previousClose')
                        if hkdusd:
                            usdhkd = round(1 / float(hkdusd), 4)
                            if 7.7 <= usdhkd <= 7.9:
                                ctx.hkd["usdhkd"] = usdhkd
                                ctx.data_sources["usdhkd"] = "Yahoo Finance"
                                hkd_found = True
            except Exception as e:
                ctx.errors.append(f"æ¸¯å…ƒ(Yahoo): {str(e)[:40]}")
        
        if not hkd_found:
            ctx.errors.append("USD/HKD: æ‰€æœ‰æ•°æ®æºå¤±è´¥")
            ctx.hkd["usdhkd"] = None  # æ˜¾å¼è®¾ç½® None
        
        # HIBOR ä»é‡‘ç®¡å±€è·å–
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            url = "https://api.hkma.gov.hk/public/market-data-and-statistics/monthly-statistical-bulletin/er-ir/hk-interbank-ir-daily"
            resp = RETRY_SESSION.get(url, headers=headers, timeout=TIMEOUT_CONFIG["hkma"], verify=False)
            if resp.status_code == 200:
                data = resp.json()
                if 'result' in data and 'records' in data['result'] and data['result']['records']:
                    latest = data['result']['records'][0]
                    if 'ir_overnight' in latest:
                        ctx.hkd["hibor_overnight"] = float(latest['ir_overnight'])
                        ctx.data_sources["hibor"] = "é¦™æ¸¯é‡‘ç®¡å±€"
                    if 'ir_1week' in latest:
                        ctx.hkd["hibor_1w"] = float(latest['ir_1week'])
                    if 'ir_1month' in latest:
                        ctx.hkd["hibor_1m"] = float(latest['ir_1month'])
            else:
                # API æˆåŠŸä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œæ˜¾å¼è®¾ç½® None
                if "hibor_overnight" not in ctx.hkd:
                    ctx.hkd["hibor_overnight"] = None
        except Exception as e:
            ctx.errors.append(f"HIBOR: {str(e)[:40]}")
            # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
            if "hibor_overnight" not in ctx.hkd:
                ctx.hkd["hibor_overnight"] = None
        
        # è®¡ç®—æ¸¯ç¾åˆ©å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None æ—¶æ‰è®¡ç®—ï¼‰
        if ctx.hkd.get("hibor_overnight") is not None and ctx.macro.get("fed_rate") is not None:
            ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        
        result = f"âœ… æ¸¯å…ƒ: {ctx.hkd.get('usdhkd', 'N/A')}"
        if ctx.hkd.get("hibor_overnight"):
            result += f", HIBOR:{ctx.hkd['hibor_overnight']}%"
        return result
        
    except Exception as e:
        ctx.errors.append(f"æ¸¯å…ƒæ•°æ®: {str(e)[:80]}")
        return "âŒ æ¸¯å…ƒæ•°æ®è·å–å¤±è´¥"


def fetch_dxy_direct(ctx: DataContext) -> bool:
    """ç›´æ¥ä»APIè·å–DXYç¾å…ƒæŒ‡æ•°ï¼ˆICEç¾å…ƒæŒ‡æ•°ï¼ŒèŒƒå›´çº¦ 90-115ï¼‰
    
    æ•°æ®æº:
    1. Yahoo Finance DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    2. ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°
    
    æ³¨æ„ï¼šä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼ˆèŒƒå›´100-130ï¼Œä¸ICE DXYä¸åŒï¼‰
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json, text/plain, */*"
    }
    
    # æ–¹æ¡ˆ1: Yahoo Finance - DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    try:
        resp = requests.get(
            "https://query1.finance.yahoo.com/v8/finance/chart/DX-Y.NYB?interval=1d&range=5d",
            headers=headers,
            timeout=TIMEOUT_CONFIG["yahoo"]
        )
        if resp.status_code == 200:
            data = resp.json()
            if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                result = data['chart']['result'][0]
                meta = result.get('meta', {})
                price = meta.get('regularMarketPrice') or meta.get('previousClose')
                if price:
                    dxy_val = round(float(price), 2)
                    if 90 <= dxy_val <= 115:  # ICE DXY æ­£å¸¸èŒƒå›´
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "Yahoo(ICE)"
                        return True
                # å¤‡é€‰ï¼šä» indicators è·å–
                indicators = result.get('indicators', {})
                quote = indicators.get('quote', [{}])[0]
                closes = quote.get('close', [])
                if closes:
                    for c in reversed(closes):
                        if c is not None:
                            dxy_val = round(float(c), 2)
                            if 90 <= dxy_val <= 115:
                                ctx.global_fx["dxy"] = dxy_val
                                ctx.data_sources["dxy"] = "Yahoo(ICE)"
                                return True
                            break
    except Exception as e:
        ctx.errors.append(f"DXY(Yahoo): {str(e)[:40]}")
    
    # æ–¹æ¡ˆ2: ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„æ¥å£ï¼‰
    try:
        import akshare as ak
        df = None
        # å°è¯•ä¸åŒçš„æ¥å£
        for method_name in ['index_global_em', 'tool_trade_date_hist_sina']:
            try:
                if hasattr(ak, method_name):
                    if method_name == 'index_global_em':
                        df = ak.index_global_em()
                    break
            except:
                continue
        
        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»å¤–æ±‡æ•°æ®ä¸­è·å–
        if df is None or df.empty:
            try:
                fx_df = ak.forex_spot_em()
                if fx_df is not None and not fx_df.empty:
                    dxy_row = fx_df[fx_df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
                    if not dxy_row.empty:
                        dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                        if 90 <= dxy_val <= 115:
                            ctx.global_fx["dxy"] = dxy_val
                            ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                            return True
            except:
                pass
        
        if df is not None and not df.empty:
            dxy_row = df[df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
            if not dxy_row.empty:
                dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                if 90 <= dxy_val <= 115:
                    ctx.global_fx["dxy"] = dxy_val
                    ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                    return True
    except Exception as e:
        ctx.errors.append(f"DXY(ä¸œæ–¹è´¢å¯Œ): {str(e)[:40]}")
    
    # ä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼Œå› ä¸ºèŒƒå›´ä¸åŒä¼šè¯¯å¯¼ç”¨æˆ·
    ctx.errors.append("DXY: ICEç¾å…ƒæŒ‡æ•°è·å–å¤±è´¥")
    ctx.global_fx["dxy"] = None  # æ˜¾å¼è®¾ç½® None
    return False


def fetch_global_fx(ctx: DataContext) -> str:
    """è·å–å…¨çƒå¤–æ±‡æ•°æ®ï¼ˆåŒ…æ‹¬DXYï¼‰"""
    try:
        import akshare as ak
        
        fx_df = None
        for attempt in range(3):
            try:
                fx_df = ak.forex_spot_em()
                if fx_df is not None and not fx_df.empty:
                    break
            except:
                if attempt < 2:
                    time.sleep(2 ** attempt)
        
        found = []
        
        if fx_df is not None and not fx_df.empty:
            pairs = {
                "EURUSD": "eurusd",
                "USDJPY": "usdjpy", 
                "GBPUSD": "gbpusd",
                "AUDUSD": "audusd",
                "USDCAD": "usdcad",
                "USDCHF": "usdchf"
            }
            
            for code, key in pairs.items():
                try:
                    row = fx_df[fx_df['ä»£ç '].str.contains(code, case=False, na=False)]
                    if not row.empty:
                        ctx.global_fx[key] = float(row['æœ€æ–°ä»·'].iloc[0])
                        found.append(code)
                except:
                    pass
            
            # å°è¯•ä»ä¸œæ–¹è´¢å¯Œè·å–DXY
            try:
                dxy_row = fx_df[fx_df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
                if not dxy_row.empty:
                    dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                    if 80 <= dxy_val <= 120:
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                        found.append("DXY")
            except:
                pass
        
        # å¦‚æœDXYè¿˜æ²¡è·å–åˆ°ï¼Œä½¿ç”¨ç›´æ¥API
        if "dxy" not in ctx.global_fx:
            if fetch_dxy_direct(ctx):
                found.append("DXY")
        
        if "dxy" not in ctx.global_fx:
            ctx.errors.append("DXY: æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥")
            ctx.global_fx["dxy"] = None  # æ˜¾å¼è®¾ç½® None
        
        return f"âœ… å…¨çƒå¤–æ±‡: {', '.join(found)}" if found else "âš ï¸ å…¨çƒå¤–æ±‡æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        # å³ä½¿AKShareå¤±è´¥ï¼Œä¹Ÿå°è¯•è·å–DXY
        if fetch_dxy_direct(ctx):
            return f"âœ… DXY: {ctx.global_fx.get('dxy')}"
        ctx.errors.append(f"å…¨çƒå¤–æ±‡: {str(e)[:80]}")
        return "âŒ å…¨çƒå¤–æ±‡è·å–å¤±è´¥"


def fetch_fred_data(ctx: DataContext) -> str:
    """è·å– FRED å®è§‚æ•°æ®"""
    fred_key = os.getenv("FRED_API_KEY")
    if not fred_key:
        ctx.errors.append("FRED_API_KEY æœªé…ç½®")
        return "âš ï¸ FRED æœªé…ç½®"
    
    try:
        from fredapi import Fred
        fred = Fred(api_key=fred_key)
        results = []
        
        try:
            us10y = fred.get_series_latest_release("DGS10")
            if us10y is not None and not us10y.empty:
                ctx.macro["us10y"] = round(float(us10y.iloc[-1]), 2)
                ctx.data_sources["us10y"] = "FRED"
                results.append("10Y")
            else:
                ctx.macro["us10y"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"US10Y: {str(e)[:40]}")
            ctx.macro["us10y"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        try:
            us2y = fred.get_series_latest_release("DGS2")
            if us2y is not None and not us2y.empty:
                ctx.macro["us2y"] = round(float(us2y.iloc[-1]), 2)
                results.append("2Y")
            else:
                ctx.macro["us2y"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"US2Y: {str(e)[:40]}")
            ctx.macro["us2y"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        # è®¡ç®—æ”¶ç›Šç‡æ›²çº¿ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None æ—¶æ‰è®¡ç®—ï¼‰
        if ctx.macro.get("us10y") is not None and ctx.macro.get("us2y") is not None:
            ctx.macro["yield_curve"] = round(ctx.macro["us10y"] - ctx.macro["us2y"], 2)
        
        try:
            vix = fred.get_series_latest_release("VIXCLS")
            if vix is not None and not vix.empty:
                vix_val = round(float(vix.iloc[-1]), 2)
                ctx.macro["vix"] = vix_val
                ctx.data_sources["vix"] = "CBOE/FRED"
                results.append("VIX")
                
                if vix_val < 15:
                    ctx.macro["market_sentiment"] = "ä¹è§‚ï¼ˆä½ææ…Œï¼‰"
                elif vix_val < 20:
                    ctx.macro["market_sentiment"] = "ä¸­æ€§"
                elif vix_val < 30:
                    ctx.macro["market_sentiment"] = "è°¨æ…"
                else:
                    ctx.macro["market_sentiment"] = "ææ…Œ"
            else:
                ctx.macro["vix"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"VIX: {str(e)[:40]}")
            ctx.macro["vix"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        try:
            ffr = fred.get_series_latest_release("FEDFUNDS")
            if ffr is not None and not ffr.empty:
                ctx.macro["fed_rate"] = round(float(ffr.iloc[-1]), 2)
                results.append("FedRate")
            else:
                ctx.macro["fed_rate"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"FedRate: {str(e)[:40]}")
            ctx.macro["fed_rate"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        return f"âœ… FRED: {', '.join(results)}" if results else "âš ï¸ FRED æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        ctx.errors.append(f"FRED: {str(e)[:80]}")
        return "âŒ FRED è·å–å¤±è´¥"


# ============================================================================
# Perplexity æ–°é—»æ£€ç´¢æ¨¡å—ï¼ˆé‡æ„ç‰ˆ v2ï¼‰
# 
# æ”¹è¿›ç‚¹ï¼š
# 1. åˆ† 3 ç±»æŸ¥è¯¢ï¼šå¤®è¡Œæ”¿ç­–ã€åœ°ç¼˜å®è§‚ã€äººæ°‘å¸æ¸¯å…ƒä¸“é¢˜
# 2. Prompt å¯¼å‘"åˆ†æ"è€Œé"æ•°æ®æ’­æŠ¥"
# 3. æ˜ç¡®æ’é™¤é‡é¸¡æº
# 4. ç®€åŒ–è§£æé€»è¾‘
# ============================================================================


# ============================================================================
# Prompt æ¨¡æ¿ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
# ============================================================================

def _get_prompt_policy(week_ago: str, today: str) -> dict:
    """å¤®è¡Œæ”¿ç­– & æ±‡ç‡åˆ†æ"""
    return {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": f"""You are an FX market analyst. Search for news from {week_ago} to {today}.

Your task: Find central bank policy news and FX market analysis.

Sources to prioritize:
- Reuters, Bloomberg, Financial Times, Wall Street Journal
- Official: Federal Reserve, ECB, BOJ, PBOC, HKMA
- Research: Goldman Sachs, Morgan Stanley, JP Morgan

Topics:
- Fed interest rate policy and dollar outlook
- Central bank policy divergence
- Major currency pair analysis (EUR/USD, USD/JPY, GBP/USD)

IMPORTANT RULES:
- Return ANALYSIS and COMMENTARY, not raw price data
- Exclude: fx168, jin10, investing.com price feeds, currency converters
- Each item must explain WHY it matters for FX markets
- Do NOT include routine daily fixing announcements

Output exactly 4 items in this format:
1. [POLICY]
TITLE: Clear headline describing the news
SUMMARY: 2-3 sentences explaining the news and its FX market impact

2. [POLICY]
TITLE: ...
SUMMARY: ..."""
            },
            {
                "role": "user",
                "content": "Find 4 important central bank policy and FX analysis news from the past week. Focus on analysis, not data."
            }
        ],
        "max_tokens": 2000,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }


def _get_prompt_geopolitical(week_ago: str, today: str) -> dict:
    """å®è§‚ & åœ°ç¼˜æ”¿æ²»"""
    return {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": f"""You are a macro strategist. Search for news from {week_ago} to {today}.

Your task: Find geopolitical and macro events that impact currency markets.

Topics to cover:
- US-China relations, trade policy, tariffs
- Regional conflicts or tensions (Middle East, Latin America, Europe, Asia)
- Sanctions, asset freezes, capital controls
- Major economic data SURPRISES (not routine releases)
- Commodity shocks affecting FX (oil, gold, copper)
- Political events (elections, policy shifts, government changes)

IMPORTANT RULES:
- Focus on events that MOVE currency markets
- Explain the FX impact, not just describe the event
- Include specific market reactions where possible
- Exclude routine economic data unless it caused significant moves

Output exactly 4 items in this format:
1. [MACRO]
TITLE: Clear headline describing the event
SUMMARY: 2-3 sentences explaining the event and its currency market impact

2. [MACRO]
TITLE: ...
SUMMARY: ..."""
            },
            {
                "role": "user",
                "content": "Find 4 major geopolitical or macro events from the past week that impacted or could impact FX markets. Explain the currency implications."
            }
        ],
        "max_tokens": 2000,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }


def _get_prompt_cny_hkd(week_ago_cn: str, today_cn: str) -> dict:
    """äººæ°‘å¸/æ¸¯å…ƒä¸“é¢˜ï¼ˆä¸­æ–‡ï¼‰"""
    return {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯å¤–æ±‡å¸‚åœºåˆ†æå¸ˆã€‚æœç´¢ {week_ago_cn} è‡³ {today_cn} çš„æ–°é—»ã€‚

ä»»åŠ¡ï¼šå¯»æ‰¾äººæ°‘å¸å’Œæ¸¯å…ƒçš„æ·±åº¦åˆ†ææŠ¥é“ã€‚

ä¼˜å…ˆæ¥æºï¼š
- è´¢æ–°ç½‘ã€ç¬¬ä¸€è´¢ç»ã€21ä¸–çºªç»æµæŠ¥é“ã€è¯åˆ¸æ—¶æŠ¥ã€ç»æµè§‚å¯ŸæŠ¥
- åˆ¸å•†ç ”æŠ¥ï¼šä¸­é‡‘å…¬å¸ã€æ‹›å•†è¯åˆ¸ã€å…´ä¸šè¯åˆ¸ã€åæ³°è¯åˆ¸ã€ä¸­ä¿¡è¯åˆ¸
- å®˜æ–¹è§£è¯»ï¼šå¤®è¡Œã€å¤–ç®¡å±€ã€é‡‘ç®¡å±€æ”¿ç­–åˆ†æ

å†…å®¹è¦æ±‚ï¼š
- æ±‡ç‡èµ°åŠ¿åˆ†æå’Œåå¸‚å±•æœ›
- æ”¿ç­–è§£è¯»ï¼ˆä¸­é—´ä»·ä¿¡å·ã€é€†å‘¨æœŸå› å­ã€MLF/LPRå½±å“ï¼‰
- èµ„é‡‘æµå‘ã€ç»“å”®æ±‡æ•°æ®åˆ†æã€å¥—æ¯äº¤æ˜“
- ç¦»å²¸åœ¨å²¸ä»·å·®åŠå…¶å«ä¹‰
- äººæ°‘å¸å›½é™…åŒ–è¿›å±•

ä¸¥æ ¼æ’é™¤ï¼š
- çº¯æ•°æ®æ’­æŠ¥ï¼ˆå¦‚"ä»Šæ—¥ä¸­é—´ä»·æŠ¥7.xxxx"ï¼‰
- æ±‡ç‡æ¢ç®—å·¥å…·ã€å¤–æ±‡ç‰Œä»·æŸ¥è¯¢é¡µé¢
- fx168ã€é‡‘åç­‰å¹³å°çš„æœºæ¢°æ•°æ®æ’­æŠ¥
- æ²¡æœ‰åˆ†æå†…å®¹çš„ä»·æ ¼å…¬å‘Š

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼4æ¡ï¼‰ï¼š
1. [CNY]
TITLE: æ¸…æ™°çš„æ–°é—»æ ‡é¢˜
SUMMARY: 2-3å¥è¯è¯´æ˜æ–°é—»å†…å®¹åŠå¸‚åœºå½±å“

2. [CNY]
TITLE: ...
SUMMARY: ..."""
            },
            {
                "role": "user",
                "content": "æœç´¢4æ¡æœ¬å‘¨äººæ°‘å¸å’Œæ¸¯å…ƒçš„æ·±åº¦åˆ†ææŠ¥é“ã€‚å¿…é¡»æ˜¯åˆ†ææ–‡ç« ï¼Œä¸è¦æ•°æ®æ’­æŠ¥ã€‚"
            }
        ],
        "max_tokens": 2000,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }


# ============================================================================
# è§£æå‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================================

def _parse_news_response(content: str, citations: list, category: str) -> List[Dict]:
    """
    è§£æ Perplexity è¿”å›çš„æ–°é—»å†…å®¹
    
    Args:
        content: API è¿”å›çš„æ–‡æœ¬å†…å®¹
        citations: å¼•ç”¨åˆ—è¡¨
        category: åˆ†ç±»æ ‡ç­¾ (POLICY/MACRO/CNY)
    
    Returns:
        æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« title, summary, urls, category
    """
    # æå–æœ‰æ•ˆ URLs
    valid_urls = []
    for c in citations:
        if isinstance(c, str) and c.startswith('http'):
            valid_urls.append(c)
        elif isinstance(c, dict):
            url = c.get('url') or c.get('link')
            if url and url.startswith('http'):
                valid_urls.append(url)
    
    news_items = []
    
    # æŒ‰æ•°å­—ç¼–å·åˆ†å‰²æ–°é—»æ¡ç›®
    # åŒ¹é…æ¨¡å¼: "1. [TAG]" æˆ– "1.[TAG]" æˆ– "1ã€[TAG]"
    pattern = r'(\d+)[\.\ã€\)]\s*\[(?:POLICY|MACRO|CNY)\]'
    parts = re.split(pattern, content)
    
    # parts ç»“æ„: ['å‰å¯¼æ–‡æœ¬', '1', 'æ–°é—»1å†…å®¹', '2', 'æ–°é—»2å†…å®¹', ...]
    i = 1
    while i < len(parts) - 1:
        news_num = parts[i]
        news_content = parts[i + 1] if i + 1 < len(parts) else ""
        
        # æå– TITLE å’Œ SUMMARY
        title = ""
        summary = ""
        
        # åŒ¹é… TITLE è¡Œ
        title_match = re.search(r'TITLE[:\sï¼š]+(.+?)(?=SUMMARY|$)', news_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            title = title_match.group(1).strip()
            # æ¸…ç†å¼•ç”¨æ ‡è®°å’Œæ¢è¡Œ
            title = re.sub(r'\s*\[\d+\]\s*', ' ', title).strip()
            title = re.sub(r'\n+', ' ', title).strip()
        
        # åŒ¹é… SUMMARY è¡Œ
        summary_match = re.search(r'SUMMARY[:\sï¼š]+(.+?)(?=\d+[\.\ã€\)]\s*\[|$)', news_content, re.IGNORECASE | re.DOTALL)
        if summary_match:
            summary = summary_match.group(1).strip()
            # æ¸…ç†å¼•ç”¨æ ‡è®°å’Œå¤šä½™æ¢è¡Œ
            summary = re.sub(r'\s*\[\d+\]\s*', ' ', summary).strip()
            summary = re.sub(r'\n+', ' ', summary).strip()
        
        # æå–å¼•ç”¨ç¼–å·
        refs = re.findall(r'\[(\d+)\]', news_content)
        urls = []
        for ref in refs:
            ref_idx = int(ref) - 1
            if 0 <= ref_idx < len(valid_urls):
                if valid_urls[ref_idx] not in urls:
                    urls.append(valid_urls[ref_idx])
        
        # åªæ·»åŠ æœ‰æ•ˆçš„æ–°é—»æ¡ç›®
        if title:
            news_items.append({
                "category": category,
                "title": title,
                "summary": summary if summary else title,
                "urls": urls[:2]  # æœ€å¤šä¿ç•™2ä¸ªURL
            })
        
        i += 2
    
    return news_items


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def fetch_perplexity_news_v2(ctx) -> str:
    """
    ä½¿ç”¨ Perplexity API è·å–å¤–æ±‡ç›¸å…³æ–°é—»ï¼ˆé‡æ„ç‰ˆï¼‰
    
    æ”¹è¿›ï¼š
    1. åˆ† 3 ç±»æŸ¥è¯¢ï¼šå¤®è¡Œæ”¿ç­–ã€åœ°ç¼˜å®è§‚ã€äººæ°‘å¸æ¸¯å…ƒ
    2. Prompt å¯¼å‘åˆ†æè€Œéæ•°æ®æ’­æŠ¥
    3. æ˜ç¡®æ’é™¤é‡é¸¡æº
    
    Args:
        ctx: DataContext å¯¹è±¡
        
    Returns:
        çŠ¶æ€æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        ctx.errors.append("PERPLEXITY_API_KEY æœªé…ç½®")
        return "âš ï¸ Perplexity æœªé…ç½®"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    today_date = datetime.now()
    week_ago = today_date - timedelta(days=7)
    today_display = today_date.strftime("%B %d, %Y")
    week_ago_display = week_ago.strftime("%B %d, %Y")
    today_cn = today_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
    week_ago_cn = week_ago.strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    # é…ç½®ä»£ç†
    proxies = None
    socks5_proxy = os.getenv("SOCKS5_PROXY")
    http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
    https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")
    
    if socks5_proxy:
        try:
            import socks
            if not socks5_proxy.startswith("socks5://"):
                socks5_url = f"socks5h://{socks5_proxy}"
            else:
                socks5_url = socks5_proxy.replace("socks5://", "socks5h://")
            proxies = {"http": socks5_url, "https": socks5_url}
        except ImportError:
            ctx.errors.append("éœ€è¦: pip install requests[socks]")
    elif http_proxy or https_proxy:
        proxies = {"http": http_proxy, "https": https_proxy or http_proxy}
    
    # å‡†å¤‡ 3 ä¸ªæŸ¥è¯¢
    queries = [
        ("POLICY", _get_prompt_policy(week_ago_display, today_display)),
        ("MACRO", _get_prompt_geopolitical(week_ago_display, today_display)),
        ("CNY", _get_prompt_cny_hkd(week_ago_cn, today_cn)),
    ]
    
    all_news = []
    stats = {"POLICY": 0, "MACRO": 0, "CNY": 0}
    
    session = requests.Session()
    
    for category, payload in queries:
        try:
            resp = session.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers,
                json=payload,
                timeout=TIMEOUT_CONFIG.get("perplexity", (30, 90)),
                verify=False,  # ä¸ºå…¼å®¹ä»£ç†ç¯å¢ƒ
                proxies=proxies
            )
            
            if resp.status_code == 200:
                result = resp.json()
                content = result['choices'][0]['message']['content']
                citations = result.get('citations', [])
                if not citations:
                    citations = result['choices'][0].get('message', {}).get('citations', [])
                
                news_items = _parse_news_response(content, citations, category)
                all_news.extend(news_items)
                stats[category] = len(news_items)
            else:
                ctx.errors.append(f"Perplexity {category}: HTTP {resp.status_code}")
                
        except Exception as e:
            ctx.errors.append(f"Perplexity {category}: {str(e)[:50]}")
    
    # å­˜å‚¨åˆ° ctx
    for item in all_news:
        # æ ¼å¼åŒ–æ ‡é¢˜ï¼š[åˆ†ç±»] æ ‡é¢˜
        formatted_title = f"[{item['category']}] {item['title']}"
        ctx.news.append(formatted_title)
        ctx.news_detail.append(item['summary'])
        ctx.news_sources.append(item['urls'])
    
    ctx.data_sources["news"] = "Perplexity(Policy+Macro+CNY)"
    
    total = len(all_news)
    return f"âœ… æ–°é—»: {total}æ¡ (æ”¿ç­–:{stats['POLICY']} å®è§‚:{stats['MACRO']} äººæ°‘å¸:{stats['CNY']})"


# ============================================================================
# å…¼å®¹æ—§æ¥å£
# ============================================================================

def fetch_perplexity_news(ctx) -> str:
    """
    å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨æ–°ç‰ˆæœ¬
    """
    return fetch_perplexity_news_v2(ctx)

def calculate_metrics(ctx: DataContext) -> str:
    results = []
    
    # è®¡ç®—æ¸¯ç¾åˆ©å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None ä¸”è¿˜æœªè®¡ç®—æ—¶ï¼‰
    if (ctx.hkd.get("hibor_overnight") is not None and 
        ctx.macro.get("fed_rate") is not None and 
        "hkd_usd_spread" not in ctx.hkd):
        ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        results.append("æ¸¯ç¾åˆ©å·®")
    
    # è®¡ç®— CNY ä»·å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None ä¸”è¿˜æœªè®¡ç®—æ—¶ï¼‰
    if (ctx.cny.get("usdcny_mid") is not None and 
        ctx.cny.get("usdcnh_spot") is not None and 
        "cny_spread" not in ctx.cny):
        ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
        results.append("CNYä»·å·®")
    
    return f"âœ… è®¡ç®—å®Œæˆ"


def retrieve_all_data(progress_callback: Optional[ProgressCallback] = None) -> DataContext:
    ctx = DataContext()
    
    steps = [
        ("FRED å®è§‚æ•°æ®", lambda: fetch_fred_data(ctx)),
        ("äººæ°‘å¸æ•°æ®", lambda: fetch_cny_data(ctx)),
        ("æ¸¯å…ƒæ•°æ®", lambda: fetch_hkd_data(ctx)),
        ("å…¨çƒå¤–æ±‡", lambda: fetch_global_fx(ctx)),
        ("Perplexity æ–°é—»", lambda: fetch_perplexity_news(ctx)),
        ("è®¡ç®—è¡ç”ŸæŒ‡æ ‡", lambda: calculate_metrics(ctx)),
    ]
    
    total = len(steps)
    
    for i, (name, func) in enumerate(steps):
        if progress_callback:
            progress_callback(i, total, f"ğŸ“Š {name}...")
        
        try:
            result = func()
            if progress_callback:
                progress_callback(i + 1, total, result)
        except Exception as e:
            ctx.errors.append(f"{name}: {str(e)[:50]}")
            if progress_callback:
                progress_callback(i + 1, total, f"âŒ {name} å¤±è´¥")
    
    return ctx


if __name__ == "__main__":
    def print_progress(step, total, msg):
        print(f"[{step}/{total}] {msg}")
    
    ctx = retrieve_all_data(print_progress)
    print("\n" + "="*60)
    print(ctx.to_json())
