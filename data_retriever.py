# data_retriever.py - æ•°æ®é‡‡é›†æ¨¡å—ï¼ˆä¿®å¤ç‰ˆ v2ï¼‰

import os
import ssl
import json
import time
import re
from datetime import datetime
from typing import Dict, Any, Optional, Callable
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from dotenv import load_dotenv

load_dotenv()

# SSLä¿®å¤
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def create_retry_session(retries=3, backoff_factor=1):
    session = requests.Session()
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

RETRY_SESSION = create_retry_session()


class DataContext:
    def __init__(self):
        self.report_date = datetime.now().strftime("%Y-%m-%d")
        self.snapshot = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.cny = {}
        self.hkd = {}
        self.global_fx = {}
        self.macro = {}
        self.news = []
        self.news_sources = []
        self.data_sources = {}
        self.errors = []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "report_date": self.report_date,
            "snapshot": self.snapshot,
            "cny": self.cny,
            "hkd": self.hkd,
            "global_fx": self.global_fx,
            "macro": self.macro,
            "news": self.news,
            "news_sources": self.news_sources,
            "data_sources": self.data_sources,
            "errors": self.errors,
            "data_points": self._count_data_points()
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
    
    def _count_data_points(self) -> int:
        count = 0
        for section in [self.cny, self.hkd, self.global_fx, self.macro]:
            count += len([v for v in section.values() if v is not None])
        count += len(self.news)
        return count


ProgressCallback = Callable[[int, int, str], None]


def fetch_cny_data(ctx: DataContext) -> str:
    """è·å–äººæ°‘å¸æ•°æ®"""
    try:
        import akshare as ak
        
        try:
            mid_df = None
            for attempt in range(3):
                try:
                    mid_df = ak.currency_boc_safe()
                    if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                usd_col = mid_df['ç¾å…ƒ'].astype(float) / 100
                ctx.cny["usdcny_mid"] = round(float(usd_col.iloc[-1]), 4)
                ctx.cny["usdcny_mid_date"] = str(mid_df['æ—¥æœŸ'].iloc[-1])
                ctx.data_sources["usdcny_mid"] = "å›½å®¶å¤–æ±‡ç®¡ç†å±€"
                
                recent = usd_col.tail(5)
                ctx.cny["usdcny_mid_range"] = f"{round(recent.min(), 4)} - {round(recent.max(), 4)}"
                ctx.cny["usdcny_mid_high"] = round(recent.max(), 4)
                ctx.cny["usdcny_mid_low"] = round(recent.min(), 4)
        except Exception as e:
            ctx.errors.append(f"äººæ°‘å¸ä¸­é—´ä»·: {str(e)[:80]}")
        
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if fx_df is not None and not fx_df.empty:
                cnh_row = fx_df[fx_df['ä»£ç '].str.contains('USDCNH', case=False, na=False)]
                if not cnh_row.empty:
                    ctx.cny["usdcnh_spot"] = float(cnh_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.data_sources["usdcnh"] = "ä¸œæ–¹è´¢å¯Œ"
                
                if ctx.cny.get("usdcny_mid") and ctx.cny.get("usdcnh_spot"):
                    ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
        except Exception as e:
            ctx.errors.append(f"ç¦»å²¸æ±‡ç‡: {str(e)[:80]}")
        
        parts = []
        if ctx.cny.get("usdcny_mid"):
            parts.append(f"ä¸­é—´ä»·:{ctx.cny['usdcny_mid']}")
        if ctx.cny.get("usdcnh_spot"):
            parts.append(f"CNH:{ctx.cny['usdcnh_spot']}")
        
        return f"âœ… äººæ°‘å¸: {', '.join(parts)}" if parts else "âš ï¸ äººæ°‘å¸æ•°æ®éƒ¨åˆ†ç¼ºå¤±"
        
    except ImportError:
        ctx.errors.append("AKShare æœªå®‰è£…")
        return "âŒ AKShare æœªå®‰è£…"
    except Exception as e:
        ctx.errors.append(f"äººæ°‘å¸æ•°æ®: {str(e)[:80]}")
        return "âŒ äººæ°‘å¸æ•°æ®è·å–å¤±è´¥"


def fetch_hkd_data(ctx: DataContext) -> str:
    """è·å–æ¸¯å…ƒæ•°æ®"""
    hkd_found = False
    
    try:
        import akshare as ak
        
        # æ–¹æ³•1: ä¸œæ–¹è´¢å¯Œå¤–æ±‡è¡Œæƒ…
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except Exception as e:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
                    else:
                        ctx.errors.append(f"ä¸œæ–¹è´¢å¯ŒAPI: {str(e)[:40]}")
            
            if fx_df is not None and not fx_df.empty:
                hkd_row = fx_df[fx_df['ä»£ç '].str.contains('USDHKD', case=False, na=False)]
                if not hkd_row.empty:
                    usdhkd = float(hkd_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.hkd["usdhkd"] = usdhkd
                    ctx.data_sources["usdhkd"] = "ä¸œæ–¹è´¢å¯Œ"
                    hkd_found = True
                    
                    if usdhkd <= 7.77:
                        ctx.hkd["lers_position"] = "å¼ºæ–¹åŒºé—´ï¼ˆæ¥è¿‘7.75å¼ºæ–¹ä¿è¯ï¼‰"
                    elif usdhkd >= 7.83:
                        ctx.hkd["lers_position"] = "å¼±æ–¹åŒºé—´ï¼ˆæ¥è¿‘7.85å¼±æ–¹ä¿è¯ï¼‰"
                    else:
                        ctx.hkd["lers_position"] = "ä¸­é—´åŒºé—´"
        except Exception as e:
            ctx.errors.append(f"æ¸¯å…ƒ(ä¸œæ–¹è´¢å¯Œ): {str(e)[:50]}")
        
        # æ–¹æ³•2: Yahoo Finance å¤‡é€‰
        if not hkd_found:
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                resp = requests.get(
                    "https://query1.finance.yahoo.com/v8/finance/chart/HKDUSD=X?interval=1d&range=1d",
                    headers=headers,
                    timeout=15
                )
                if resp.status_code == 200:
                    data = resp.json()
                    if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                        meta = data['chart']['result'][0].get('meta', {})
                        hkdusd = meta.get('regularMarketPrice') or meta.get('previousClose')
                        if hkdusd:
                            usdhkd = round(1 / float(hkdusd), 4)
                            if 7.7 <= usdhkd <= 7.9:
                                ctx.hkd["usdhkd"] = usdhkd
                                ctx.data_sources["usdhkd"] = "Yahoo Finance"
                                hkd_found = True
            except Exception as e:
                ctx.errors.append(f"æ¸¯å…ƒ(Yahoo): {str(e)[:40]}")
        
        if not hkd_found:
            ctx.errors.append("USD/HKD: æ‰€æœ‰æ•°æ®æºå¤±è´¥")
        
        # HIBOR ä»é‡‘ç®¡å±€è·å–
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            url = "https://api.hkma.gov.hk/public/market-data-and-statistics/monthly-statistical-bulletin/er-ir/hk-interbank-ir-daily"
            resp = RETRY_SESSION.get(url, headers=headers, timeout=30, verify=False)
            if resp.status_code == 200:
                data = resp.json()
                if 'result' in data and 'records' in data['result'] and data['result']['records']:
                    latest = data['result']['records'][0]
                    if 'ir_overnight' in latest:
                        ctx.hkd["hibor_overnight"] = float(latest['ir_overnight'])
                        ctx.data_sources["hibor"] = "é¦™æ¸¯é‡‘ç®¡å±€"
                    if 'ir_1week' in latest:
                        ctx.hkd["hibor_1w"] = float(latest['ir_1week'])
                    if 'ir_1month' in latest:
                        ctx.hkd["hibor_1m"] = float(latest['ir_1month'])
        except Exception as e:
            ctx.errors.append(f"HIBOR: {str(e)[:40]}")
        
        if ctx.hkd.get("hibor_overnight") and ctx.macro.get("fed_rate"):
            ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        
        result = f"âœ… æ¸¯å…ƒ: {ctx.hkd.get('usdhkd', 'N/A')}"
        if ctx.hkd.get("hibor_overnight"):
            result += f", HIBOR:{ctx.hkd['hibor_overnight']}%"
        return result
        
    except Exception as e:
        ctx.errors.append(f"æ¸¯å…ƒæ•°æ®: {str(e)[:80]}")
        return "âŒ æ¸¯å…ƒæ•°æ®è·å–å¤±è´¥"


def fetch_dxy_direct(ctx: DataContext) -> bool:
    """ç›´æ¥ä»APIè·å–DXYç¾å…ƒæŒ‡æ•°ï¼ˆICEç¾å…ƒæŒ‡æ•°ï¼ŒèŒƒå›´çº¦ 90-115ï¼‰
    
    æ•°æ®æº:
    1. Yahoo Finance DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    2. ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°
    
    æ³¨æ„ï¼šä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼ˆèŒƒå›´100-130ï¼Œä¸ICE DXYä¸åŒï¼‰
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json, text/plain, */*"
    }
    
    # æ–¹æ¡ˆ1: Yahoo Finance - DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    try:
        resp = requests.get(
            "https://query1.finance.yahoo.com/v8/finance/chart/DX-Y.NYB?interval=1d&range=5d",
            headers=headers,
            timeout=15
        )
        if resp.status_code == 200:
            data = resp.json()
            if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                result = data['chart']['result'][0]
                meta = result.get('meta', {})
                price = meta.get('regularMarketPrice') or meta.get('previousClose')
                if price:
                    dxy_val = round(float(price), 2)
                    if 90 <= dxy_val <= 115:  # ICE DXY æ­£å¸¸èŒƒå›´
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "Yahoo(ICE)"
                        return True
                # å¤‡é€‰ï¼šä» indicators è·å–
                indicators = result.get('indicators', {})
                quote = indicators.get('quote', [{}])[0]
                closes = quote.get('close', [])
                if closes:
                    for c in reversed(closes):
                        if c is not None:
                            dxy_val = round(float(c), 2)
                            if 90 <= dxy_val <= 115:
                                ctx.global_fx["dxy"] = dxy_val
                                ctx.data_sources["dxy"] = "Yahoo(ICE)"
                                return True
                            break
    except Exception as e:
        ctx.errors.append(f"DXY(Yahoo): {str(e)[:40]}")
    
    # æ–¹æ¡ˆ2: ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°
    try:
        import akshare as ak
        df = ak.index_global_em()
        if df is not None and not df.empty:
            dxy_row = df[df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
            if not dxy_row.empty:
                dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                if 90 <= dxy_val <= 115:
                    ctx.global_fx["dxy"] = dxy_val
                    ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                    return True
    except Exception as e:
        ctx.errors.append(f"DXY(ä¸œæ–¹è´¢å¯Œ): {str(e)[:40]}")
    
    # ä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼Œå› ä¸ºèŒƒå›´ä¸åŒä¼šè¯¯å¯¼ç”¨æˆ·
    ctx.errors.append("DXY: ICEç¾å…ƒæŒ‡æ•°è·å–å¤±è´¥")
    return False


def fetch_global_fx(ctx: DataContext) -> str:
    """è·å–å…¨çƒå¤–æ±‡æ•°æ®ï¼ˆåŒ…æ‹¬DXYï¼‰"""
    try:
        import akshare as ak
        
        fx_df = None
        for attempt in range(3):
            try:
                fx_df = ak.forex_spot_em()
                if fx_df is not None and not fx_df.empty:
                    break
            except:
                if attempt < 2:
                    time.sleep(2 ** attempt)
        
        found = []
        
        if fx_df is not None and not fx_df.empty:
            pairs = {
                "EURUSD": "eurusd",
                "USDJPY": "usdjpy", 
                "GBPUSD": "gbpusd",
                "AUDUSD": "audusd",
                "USDCAD": "usdcad",
                "USDCHF": "usdchf"
            }
            
            for code, key in pairs.items():
                try:
                    row = fx_df[fx_df['ä»£ç '].str.contains(code, case=False, na=False)]
                    if not row.empty:
                        ctx.global_fx[key] = float(row['æœ€æ–°ä»·'].iloc[0])
                        found.append(code)
                except:
                    pass
            
            # å°è¯•ä»ä¸œæ–¹è´¢å¯Œè·å–DXY
            try:
                dxy_row = fx_df[fx_df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
                if not dxy_row.empty:
                    dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                    if 80 <= dxy_val <= 120:
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                        found.append("DXY")
            except:
                pass
        
        # å¦‚æœDXYè¿˜æ²¡è·å–åˆ°ï¼Œä½¿ç”¨ç›´æ¥API
        if "dxy" not in ctx.global_fx:
            if fetch_dxy_direct(ctx):
                found.append("DXY")
        
        if "dxy" not in ctx.global_fx:
            ctx.errors.append("DXY: æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥")
        
        return f"âœ… å…¨çƒå¤–æ±‡: {', '.join(found)}" if found else "âš ï¸ å…¨çƒå¤–æ±‡æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        # å³ä½¿AKShareå¤±è´¥ï¼Œä¹Ÿå°è¯•è·å–DXY
        if fetch_dxy_direct(ctx):
            return f"âœ… DXY: {ctx.global_fx.get('dxy')}"
        ctx.errors.append(f"å…¨çƒå¤–æ±‡: {str(e)[:80]}")
        return "âŒ å…¨çƒå¤–æ±‡è·å–å¤±è´¥"


def fetch_fred_data(ctx: DataContext) -> str:
    """è·å– FRED å®è§‚æ•°æ®"""
    fred_key = os.getenv("FRED_API_KEY")
    if not fred_key:
        ctx.errors.append("FRED_API_KEY æœªé…ç½®")
        return "âš ï¸ FRED æœªé…ç½®"
    
    try:
        from fredapi import Fred
        fred = Fred(api_key=fred_key)
        results = []
        
        try:
            us10y = fred.get_series_latest_release("DGS10")
            if us10y is not None and not us10y.empty:
                ctx.macro["us10y"] = round(float(us10y.iloc[-1]), 2)
                ctx.data_sources["us10y"] = "FRED"
                results.append("10Y")
        except:
            pass
        
        try:
            us2y = fred.get_series_latest_release("DGS2")
            if us2y is not None and not us2y.empty:
                ctx.macro["us2y"] = round(float(us2y.iloc[-1]), 2)
                results.append("2Y")
        except:
            pass
        
        if ctx.macro.get("us10y") and ctx.macro.get("us2y"):
            ctx.macro["yield_curve"] = round(ctx.macro["us10y"] - ctx.macro["us2y"], 2)
        
        try:
            vix = fred.get_series_latest_release("VIXCLS")
            if vix is not None and not vix.empty:
                vix_val = round(float(vix.iloc[-1]), 2)
                ctx.macro["vix"] = vix_val
                ctx.data_sources["vix"] = "CBOE/FRED"
                results.append("VIX")
                
                if vix_val < 15:
                    ctx.macro["market_sentiment"] = "ä¹è§‚ï¼ˆä½ææ…Œï¼‰"
                elif vix_val < 20:
                    ctx.macro["market_sentiment"] = "ä¸­æ€§"
                elif vix_val < 30:
                    ctx.macro["market_sentiment"] = "è°¨æ…"
                else:
                    ctx.macro["market_sentiment"] = "ææ…Œ"
        except:
            pass
        
        try:
            ffr = fred.get_series_latest_release("FEDFUNDS")
            if ffr is not None and not ffr.empty:
                ctx.macro["fed_rate"] = round(float(ffr.iloc[-1]), 2)
                results.append("FedRate")
        except:
            pass
        
        return f"âœ… FRED: {', '.join(results)}" if results else "âš ï¸ FRED æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        ctx.errors.append(f"FRED: {str(e)[:80]}")
        return "âŒ FRED è·å–å¤±è´¥"


def fetch_perplexity_news(ctx: DataContext) -> str:
    """ä½¿ç”¨ Perplexity API è·å–å¤–æ±‡ç›¸å…³æ–°é—»
    
    èŒƒå›´ï¼šäººæ°‘å¸/æ¸¯å…ƒ/G10è´§å¸å¯¹ç¾å…ƒã€å„å›½å¤®è¡Œæ”¿ç­–
    æ¥æºï¼šæƒå¨è´¢ç»åª’ä½“+å¤®è¡Œå®˜ç½‘
    """
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        ctx.errors.append("PERPLEXITY_API_KEY æœªé…ç½®")
        return "âš ï¸ Perplexity æœªé…ç½®"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    payload = {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸“ä¸šå¤–æ±‡å¸‚åœºæ–°é—»ç¼–è¾‘ã€‚

ã€æ–°é—»èŒƒå›´ã€‘
1. äººæ°‘å¸(USD/CNY, USD/CNH)ï¼šä¸­é—´ä»·ã€ç¦»å²¸åœ¨å²¸ä»·å·®ã€ä¸­å›½å¤®è¡Œ(PBOC)ã€å¤–ç®¡å±€(SAFE)æ”¿ç­–
2. æ¸¯å…ƒ(USD/HKD)ï¼šè”ç³»æ±‡ç‡ã€é¦™æ¸¯é‡‘ç®¡å±€(HKMA)æ“ä½œã€HIBOR
3. G10è´§å¸å¯¹ç¾å…ƒï¼šEUR/USDã€USD/JPYã€GBP/USDã€AUD/USDã€USD/CADã€USD/CHFç­‰
4. å„å›½å¤®è¡Œæ”¿ç­–ï¼šç¾è”å‚¨(Fed/FOMC)ã€æ¬§å¤®è¡Œ(ECB)ã€æ—¥æœ¬å¤®è¡Œ(BOJ)ã€è‹±å›½å¤®è¡Œ(BOE)ç­‰
5. ç¾å…ƒæŒ‡æ•°(DXY)ã€ç¾å€ºæ”¶ç›Šç‡ã€VIXã€é£é™©æƒ…ç»ª

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¾“å‡ºæ–°é—»å†…å®¹ï¼Œæ¯æ¡ä¸€è¡Œï¼Œç”¨æ•°å­—ç¼–å·ã€‚ä¸è¦æ ‡æ³¨æ¥æºåç§°ã€‚

ç¤ºä¾‹ï¼š
1. ç¾è”å‚¨12æœˆFOMCä¼šè®®å®£å¸ƒé™æ¯25åŸºç‚¹è‡³4.25%-4.50%ï¼Œä½†ç‚¹é˜µå›¾æ˜¾ç¤º2025å¹´ä»…é¢„æœŸé™æ¯ä¸¤æ¬¡
2. ä¸­å›½å¤®è¡Œå°†äººæ°‘å¸ä¸­é—´ä»·è®¾å®šä¸º7.1876ï¼Œè¿ç»­ç¬¬ä¸‰æ—¥ç»´æŒåœ¨7.19ä¸‹æ–¹
3. é¦™æ¸¯é‡‘ç®¡å±€å…¥å¸‚ä¹°å…¥18.46äº¿æ¸¯å…ƒï¼Œä¸ºæœ¬æœˆç¬¬å››æ¬¡æå«è”ç³»æ±‡ç‡

æ³¨æ„ï¼š
- æ‰€æœ‰è´§å¸å¯¹ä»¥ç¾å…ƒä¸ºåŸºå‡†ï¼ˆUSD/JPYï¼Œä¸è¦JPY/USDï¼‰
- åªæŠ¥é“äº‹å®ï¼Œä¸è¦åŠ "æ®XXæŠ¥é“"è¿™ç±»æ¥æºæ ‡æ³¨
- æ–°é—»è¦å…·ä½“ã€æœ‰æ•°æ®æ”¯æ’‘"""
            },
            {
                "role": "user", 
                "content": f"""æœç´¢{today}å‰åä¸€å‘¨çš„å¤–æ±‡å¸‚åœºé‡è¦æ–°é—»ï¼š

1. ç¾è”å‚¨/FOMCæœ€æ–°æ”¿ç­–å’Œå®˜å‘˜è®²è¯
2. ä¸­å›½å¤®è¡Œ/å¤–ç®¡å±€æ”¿ç­–ã€äººæ°‘å¸ä¸­é—´ä»·å’Œæ±‡ç‡èµ°åŠ¿
3. é¦™æ¸¯é‡‘ç®¡å±€æ“ä½œã€æ¸¯å…ƒå’ŒHIBORåŠ¨æ€
4. å…¶ä»–G10è´§å¸ï¼ˆEUR/USDã€USD/JPYã€GBP/USDç­‰ï¼‰é‡å¤§å˜åŠ¨
5. å½±å“æ±‡å¸‚çš„å®è§‚æ•°æ®å’Œé£é™©äº‹ä»¶

åˆ—å‡º10-12æ¡æœ€é‡è¦çš„æ–°é—»ï¼Œè¦æ±‚å…·ä½“ã€æœ‰æ•°æ®ã€‚"""
            }
        ],
        "max_tokens": 2500,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }
    
    # é…ç½®ä»£ç†
    proxies = None
    socks5_proxy = os.getenv("SOCKS5_PROXY")
    http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
    https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")
    
    if socks5_proxy:
        try:
            import socks
            if not socks5_proxy.startswith("socks5://"):
                socks5_url = f"socks5h://{socks5_proxy}"
            else:
                socks5_url = socks5_proxy.replace("socks5://", "socks5h://")
            proxies = {"http": socks5_url, "https": socks5_url}
            ctx.data_sources["perplexity_proxy"] = f"SOCKS5"
        except ImportError:
            ctx.errors.append("éœ€è¦: pip install requests[socks]")
            proxies = None
    elif http_proxy or https_proxy:
        proxies = {"http": http_proxy, "https": https_proxy or http_proxy}
        ctx.data_sources["perplexity_proxy"] = "HTTPä»£ç†"
    else:
        ctx.data_sources["perplexity_proxy"] = "ç›´è¿"
    
    try:
        session = requests.Session()
        response = session.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=payload,
            timeout=(30, 120),
            verify=False,
            proxies=proxies
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            # Perplexity çš„ citations æ˜¯ä¸€ä¸ªURLæ•°ç»„
            citations = result.get('citations', [])
            
            lines = content.strip().split('\n')
            news_count = 0
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('---') or line.startswith('ã€'):
                    continue
                
                # æ¸…ç†è¡Œé¦–çš„ç¼–å·å’Œç¬¦å·
                news_content = re.sub(r'^[\d]+[.ã€)\]\s]+', '', line).strip()
                news_content = news_content.lstrip('â€¢*- ').strip()
                
                if news_content and len(news_content) > 20:
                    # æ–¹æ³•1: ä»æ–°é—»å†…å®¹ä¸­æå–å¼•ç”¨æ ‡è®° [1], [2][3] ç­‰
                    ref_matches = re.findall(r'\[(\d+)\]', news_content)
                    
                    urls = []
                    if ref_matches:
                        # æœ‰å¼•ç”¨æ ‡è®°ï¼ŒæŒ‰æ ‡è®°åŒ¹é…
                        for ref in ref_matches:
                            ref_idx = int(ref) - 1
                            if 0 <= ref_idx < len(citations):
                                citation = citations[ref_idx]
                                if isinstance(citation, str) and citation.startswith('http'):
                                    if citation not in urls:
                                        urls.append(citation)
                        # æ¸…ç†å¼•ç”¨æ ‡è®°
                        news_content = re.sub(r'\s*\[\d+\]\s*', ' ', news_content).strip()
                    else:
                        # æ–¹æ³•2: æ²¡æœ‰å¼•ç”¨æ ‡è®°ï¼ŒæŒ‰é¡ºåºåˆ†é… citations
                        if news_count < len(citations):
                            citation = citations[news_count]
                            if isinstance(citation, str) and citation.startswith('http'):
                                urls.append(citation)
                    
                    ctx.news.append(news_content)
                    ctx.news_sources.append(urls if urls else [])
                    news_count += 1
            
            ctx.data_sources["news"] = "Perplexityæœç´¢"
            ctx.data_sources["news_citations_count"] = len(citations)
            return f"âœ… æ–°é—»: {news_count} æ¡ (å¼•ç”¨æº: {len(citations)}ä¸ª)"
        else:
            error_msg = f"APIé”™è¯¯ {response.status_code}"
            try:
                err_json = response.json()
                error_msg += f": {err_json.get('error', {}).get('message', '')[:50]}"
            except:
                pass
            ctx.errors.append(f"Perplexity: {error_msg}")
            return f"âŒ Perplexity: {error_msg}"
            
    except requests.exceptions.Timeout:
        ctx.errors.append("Perplexity: è¿æ¥è¶…æ—¶")
        return "âš ï¸ Perplexity: è¶…æ—¶"
    except Exception as e:
        ctx.errors.append(f"Perplexity: {str(e)[:60]}")
        return f"âš ï¸ Perplexity: {str(e)[:40]}"


def calculate_metrics(ctx: DataContext) -> str:
    results = []
    
    if ctx.hkd.get("hibor_overnight") and ctx.macro.get("fed_rate") and not ctx.hkd.get("hkd_usd_spread"):
        ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        results.append("æ¸¯ç¾åˆ©å·®")
    
    if ctx.cny.get("usdcny_mid") and ctx.cny.get("usdcnh_spot") and not ctx.cny.get("cny_spread"):
        ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
        results.append("CNYä»·å·®")
    
    return f"âœ… è®¡ç®—å®Œæˆ"


def retrieve_all_data(progress_callback: Optional[ProgressCallback] = None) -> DataContext:
    ctx = DataContext()
    
    steps = [
        ("FRED å®è§‚æ•°æ®", lambda: fetch_fred_data(ctx)),
        ("äººæ°‘å¸æ•°æ®", lambda: fetch_cny_data(ctx)),
        ("æ¸¯å…ƒæ•°æ®", lambda: fetch_hkd_data(ctx)),
        ("å…¨çƒå¤–æ±‡", lambda: fetch_global_fx(ctx)),
        ("Perplexity æ–°é—»", lambda: fetch_perplexity_news(ctx)),
        ("è®¡ç®—è¡ç”ŸæŒ‡æ ‡", lambda: calculate_metrics(ctx)),
    ]
    
    total = len(steps)
    
    for i, (name, func) in enumerate(steps):
        if progress_callback:
            progress_callback(i, total, f"ğŸ“Š {name}...")
        
        try:
            result = func()
            if progress_callback:
                progress_callback(i + 1, total, result)
        except Exception as e:
            ctx.errors.append(f"{name}: {str(e)[:50]}")
            if progress_callback:
                progress_callback(i + 1, total, f"âŒ {name} å¤±è´¥")
    
    return ctx


if __name__ == "__main__":
    def print_progress(step, total, msg):
        print(f"[{step}/{total}] {msg}")
    
    ctx = retrieve_all_data(print_progress)
    print("\n" + "="*60)
    print(ctx.to_json())
