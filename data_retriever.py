# data_retriever.py - æ•°æ®é‡‡é›†æ¨¡å—ï¼ˆä¿®å¤ç‰ˆ v2ï¼‰

import os
import ssl
import json
import time
import re
from datetime import datetime
from typing import Dict, Any, Optional, Callable
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from dotenv import load_dotenv

load_dotenv()

# P0-2: å¯¼å…¥è¶…æ—¶é…ç½®; P1: å¯¼å…¥ç¼“å­˜ TTL é…ç½®
try:
    from config import TIMEOUT_CONFIG, CACHE_TTL
except ImportError:
    # å¦‚æœ config.py æœªæ›´æ–°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    TIMEOUT_CONFIG = {
        "default": (10, 30),
        "akshare": (10, 20),
        "fred": (10, 30),
        "perplexity": (30, 90),
        "yahoo": (10, 15),
        "hkma": (10, 20),
    }
    CACHE_TTL = {
        "cny_mid": 3600,
        "cny_spot": 60,
        "hkd": 60,
        "fred": 300,
        "global_fx": 60,
        "news": 600,
    }

# SSLä¿®å¤
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def create_retry_session(retries=3, backoff_factor=1):
    session = requests.Session()
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

RETRY_SESSION = create_retry_session()


# ============================================================================
# P1: ç®€å•ç¼“å­˜æ¨¡å—
# ============================================================================

import time as _time
from typing import TypeVar

_cache: Dict[str, Any] = {}
_cache_time: Dict[str, float] = {}

T = TypeVar('T')

def get_with_cache(key: str, fetch_func: Callable[[], T], ttl_seconds: int) -> T:
    """
    å¸¦ TTL çš„ç®€å•ç¼“å­˜
    
    Args:
        key: ç¼“å­˜é”®å
        fetch_func: è·å–æ•°æ®çš„å‡½æ•°
        ttl_seconds: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        
    Returns:
        ç¼“å­˜çš„æ•°æ®æˆ–æ–°è·å–çš„æ•°æ®
        
    TTL æ¨èå€¼ï¼ˆå‚è€ƒ config.CACHE_TTLï¼‰:
    - ä¸­é—´ä»· (cny_mid): 3600ç§’ï¼ˆ9:15å‘å¸ƒåæ•´å¤©ä¸å˜ï¼‰
    - å®æ—¶æ±‡ç‡ (cny_spot): 60ç§’
    - FRED æ•°æ®: 300ç§’
    - æ–°é—»: 600ç§’
    """
    now = _time.time()
    if key in _cache and (now - _cache_time.get(key, 0)) < ttl_seconds:
        return _cache[key]
    
    result = fetch_func()
    _cache[key] = result
    _cache_time[key] = now
    return result


def clear_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆç”¨äºå¼ºåˆ¶åˆ·æ–°ï¼‰"""
    global _cache, _cache_time
    _cache = {}
    _cache_time = {}


class DataContext:
    def __init__(self):
        self.report_date = datetime.now().strftime("%Y-%m-%d")
        self.snapshot = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.cny = {}
        self.hkd = {}
        self.global_fx = {}
        self.macro = {}
        self.news = []  # çŸ­æ ‡é¢˜åˆ—è¡¨ï¼ˆç”¨äºé¡µé¢å±•ç¤ºï¼‰
        self.news_detail = []  # è¯¦ç»†æ‘˜è¦åˆ—è¡¨ï¼ˆç”¨äºLLMç”ŸæˆæŠ¥å‘Šï¼‰
        self.news_sources = []
        self.data_sources = {}
        self.errors = []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "report_date": self.report_date,
            "snapshot": self.snapshot,
            "cny": self.cny,
            "hkd": self.hkd,
            "global_fx": self.global_fx,
            "macro": self.macro,
            "news": self.news,
            "news_detail": self.news_detail,
            "news_sources": self.news_sources,
            "data_sources": self.data_sources,
            "errors": self.errors,
            "data_points": self._count_data_points()
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
    
    def _count_data_points(self) -> int:
        count = 0
        for section in [self.cny, self.hkd, self.global_fx, self.macro]:
            count += len([v for v in section.values() if v is not None])
        count += len(self.news)
        return count


ProgressCallback = Callable[[int, int, str], None]


def fetch_cny_data(ctx: DataContext) -> str:
    """è·å–äººæ°‘å¸æ•°æ®"""
    try:
        import akshare as ak
        
        try:
            mid_df = None
            for attempt in range(3):
                try:
                    mid_df = ak.currency_boc_safe()
                    if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if mid_df is not None and not mid_df.empty and 'ç¾å…ƒ' in mid_df.columns:
                usd_col = mid_df['ç¾å…ƒ'].astype(float) / 100
                ctx.cny["usdcny_mid"] = round(float(usd_col.iloc[-1]), 4)
                ctx.cny["usdcny_mid_date"] = str(mid_df['æ—¥æœŸ'].iloc[-1])
                ctx.data_sources["usdcny_mid"] = "å›½å®¶å¤–æ±‡ç®¡ç†å±€"
                
                recent = usd_col.tail(5)
                ctx.cny["usdcny_mid_range"] = f"{round(recent.min(), 4)} - {round(recent.max(), 4)}"
                ctx.cny["usdcny_mid_high"] = round(recent.max(), 4)
                ctx.cny["usdcny_mid_low"] = round(recent.min(), 4)
            else:
                # API å¤±è´¥æˆ–æ•°æ®æ— æ•ˆï¼Œæ˜¾å¼è®¾ç½® None
                ctx.cny["usdcny_mid"] = None
        except Exception as e:
            ctx.errors.append(f"äººæ°‘å¸ä¸­é—´ä»·: {str(e)[:80]}")
            ctx.cny["usdcny_mid"] = None
        
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
            
            if fx_df is not None and not fx_df.empty:
                cnh_row = fx_df[fx_df['ä»£ç '].str.contains('USDCNH', case=False, na=False)]
                if not cnh_row.empty:
                    ctx.cny["usdcnh_spot"] = float(cnh_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.data_sources["usdcnh"] = "ä¸œæ–¹è´¢å¯Œ"
                else:
                    # æœªæ‰¾åˆ° USDCNH æ•°æ®
                    ctx.cny["usdcnh_spot"] = None
                # è®¡ç®—ä»·å·®ï¼ˆå¦‚æœä¸¤ä¸ªå€¼éƒ½å­˜åœ¨ï¼‰
                if ctx.cny.get("usdcny_mid") is not None and ctx.cny.get("usdcnh_spot") is not None:
                    ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
            else:
                # API å¤±è´¥æˆ–æ•°æ®æ— æ•ˆï¼Œæ˜¾å¼è®¾ç½® None
                ctx.cny["usdcnh_spot"] = None
        except Exception as e:
            ctx.errors.append(f"ç¦»å²¸æ±‡ç‡: {str(e)[:80]}")
            ctx.cny["usdcnh_spot"] = None
        
        parts = []
        if ctx.cny.get("usdcny_mid"):
            parts.append(f"ä¸­é—´ä»·:{ctx.cny['usdcny_mid']}")
        if ctx.cny.get("usdcnh_spot"):
            parts.append(f"CNH:{ctx.cny['usdcnh_spot']}")
        
        return f"âœ… äººæ°‘å¸: {', '.join(parts)}" if parts else "âš ï¸ äººæ°‘å¸æ•°æ®éƒ¨åˆ†ç¼ºå¤±"
        
    except ImportError:
        ctx.errors.append("AKShare æœªå®‰è£…")
        return "âŒ AKShare æœªå®‰è£…"
    except Exception as e:
        ctx.errors.append(f"äººæ°‘å¸æ•°æ®: {str(e)[:80]}")
        return "âŒ äººæ°‘å¸æ•°æ®è·å–å¤±è´¥"


def fetch_hkd_data(ctx: DataContext) -> str:
    """è·å–æ¸¯å…ƒæ•°æ®"""
    hkd_found = False
    
    try:
        import akshare as ak
        
        # æ–¹æ³•1: ä¸œæ–¹è´¢å¯Œå¤–æ±‡è¡Œæƒ…
        try:
            fx_df = None
            for attempt in range(3):
                try:
                    fx_df = ak.forex_spot_em()
                    if fx_df is not None and not fx_df.empty:
                        break
                except Exception as e:
                    if attempt < 2:
                        time.sleep(2 ** attempt)
                    else:
                        ctx.errors.append(f"ä¸œæ–¹è´¢å¯ŒAPI: {str(e)[:40]}")
            
            if fx_df is not None and not fx_df.empty:
                hkd_row = fx_df[fx_df['ä»£ç '].str.contains('USDHKD', case=False, na=False)]
                if not hkd_row.empty:
                    usdhkd = float(hkd_row['æœ€æ–°ä»·'].iloc[0])
                    ctx.hkd["usdhkd"] = usdhkd
                    ctx.data_sources["usdhkd"] = "ä¸œæ–¹è´¢å¯Œ"
                    hkd_found = True
                    
                    if usdhkd <= 7.77:
                        ctx.hkd["lers_position"] = "å¼ºæ–¹åŒºé—´ï¼ˆæ¥è¿‘7.75å¼ºæ–¹ä¿è¯ï¼‰"
                    elif usdhkd >= 7.83:
                        ctx.hkd["lers_position"] = "å¼±æ–¹åŒºé—´ï¼ˆæ¥è¿‘7.85å¼±æ–¹ä¿è¯ï¼‰"
                    else:
                        ctx.hkd["lers_position"] = "ä¸­é—´åŒºé—´"
        except Exception as e:
            ctx.errors.append(f"æ¸¯å…ƒ(ä¸œæ–¹è´¢å¯Œ): {str(e)[:50]}")
        
        # æ–¹æ³•2: Yahoo Finance å¤‡é€‰
        if not hkd_found:
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                resp = requests.get(
                    "https://query1.finance.yahoo.com/v8/finance/chart/HKDUSD=X?interval=1d&range=1d",
                    headers=headers,
                    timeout=TIMEOUT_CONFIG["yahoo"]
                )
                if resp.status_code == 200:
                    data = resp.json()
                    if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                        meta = data['chart']['result'][0].get('meta', {})
                        hkdusd = meta.get('regularMarketPrice') or meta.get('previousClose')
                        if hkdusd:
                            usdhkd = round(1 / float(hkdusd), 4)
                            if 7.7 <= usdhkd <= 7.9:
                                ctx.hkd["usdhkd"] = usdhkd
                                ctx.data_sources["usdhkd"] = "Yahoo Finance"
                                hkd_found = True
            except Exception as e:
                ctx.errors.append(f"æ¸¯å…ƒ(Yahoo): {str(e)[:40]}")
        
        if not hkd_found:
            ctx.errors.append("USD/HKD: æ‰€æœ‰æ•°æ®æºå¤±è´¥")
            ctx.hkd["usdhkd"] = None  # æ˜¾å¼è®¾ç½® None
        
        # HIBOR ä»é‡‘ç®¡å±€è·å–
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            url = "https://api.hkma.gov.hk/public/market-data-and-statistics/monthly-statistical-bulletin/er-ir/hk-interbank-ir-daily"
            resp = RETRY_SESSION.get(url, headers=headers, timeout=TIMEOUT_CONFIG["hkma"], verify=False)
            if resp.status_code == 200:
                data = resp.json()
                if 'result' in data and 'records' in data['result'] and data['result']['records']:
                    latest = data['result']['records'][0]
                    if 'ir_overnight' in latest:
                        ctx.hkd["hibor_overnight"] = float(latest['ir_overnight'])
                        ctx.data_sources["hibor"] = "é¦™æ¸¯é‡‘ç®¡å±€"
                    if 'ir_1week' in latest:
                        ctx.hkd["hibor_1w"] = float(latest['ir_1week'])
                    if 'ir_1month' in latest:
                        ctx.hkd["hibor_1m"] = float(latest['ir_1month'])
            else:
                # API æˆåŠŸä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œæ˜¾å¼è®¾ç½® None
                if "hibor_overnight" not in ctx.hkd:
                    ctx.hkd["hibor_overnight"] = None
        except Exception as e:
            ctx.errors.append(f"HIBOR: {str(e)[:40]}")
            # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
            if "hibor_overnight" not in ctx.hkd:
                ctx.hkd["hibor_overnight"] = None
        
        # è®¡ç®—æ¸¯ç¾åˆ©å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None æ—¶æ‰è®¡ç®—ï¼‰
        if ctx.hkd.get("hibor_overnight") is not None and ctx.macro.get("fed_rate") is not None:
            ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        
        result = f"âœ… æ¸¯å…ƒ: {ctx.hkd.get('usdhkd', 'N/A')}"
        if ctx.hkd.get("hibor_overnight"):
            result += f", HIBOR:{ctx.hkd['hibor_overnight']}%"
        return result
        
    except Exception as e:
        ctx.errors.append(f"æ¸¯å…ƒæ•°æ®: {str(e)[:80]}")
        return "âŒ æ¸¯å…ƒæ•°æ®è·å–å¤±è´¥"


def fetch_dxy_direct(ctx: DataContext) -> bool:
    """ç›´æ¥ä»APIè·å–DXYç¾å…ƒæŒ‡æ•°ï¼ˆICEç¾å…ƒæŒ‡æ•°ï¼ŒèŒƒå›´çº¦ 90-115ï¼‰
    
    æ•°æ®æº:
    1. Yahoo Finance DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    2. ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°
    
    æ³¨æ„ï¼šä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼ˆèŒƒå›´100-130ï¼Œä¸ICE DXYä¸åŒï¼‰
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json, text/plain, */*"
    }
    
    # æ–¹æ¡ˆ1: Yahoo Finance - DX-Y.NYB (ICEç¾å…ƒæŒ‡æ•°æœŸè´§)
    try:
        resp = requests.get(
            "https://query1.finance.yahoo.com/v8/finance/chart/DX-Y.NYB?interval=1d&range=5d",
            headers=headers,
            timeout=TIMEOUT_CONFIG["yahoo"]
        )
        if resp.status_code == 200:
            data = resp.json()
            if 'chart' in data and 'result' in data['chart'] and data['chart']['result']:
                result = data['chart']['result'][0]
                meta = result.get('meta', {})
                price = meta.get('regularMarketPrice') or meta.get('previousClose')
                if price:
                    dxy_val = round(float(price), 2)
                    if 90 <= dxy_val <= 115:  # ICE DXY æ­£å¸¸èŒƒå›´
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "Yahoo(ICE)"
                        return True
                # å¤‡é€‰ï¼šä» indicators è·å–
                indicators = result.get('indicators', {})
                quote = indicators.get('quote', [{}])[0]
                closes = quote.get('close', [])
                if closes:
                    for c in reversed(closes):
                        if c is not None:
                            dxy_val = round(float(c), 2)
                            if 90 <= dxy_val <= 115:
                                ctx.global_fx["dxy"] = dxy_val
                                ctx.data_sources["dxy"] = "Yahoo(ICE)"
                                return True
                            break
    except Exception as e:
        ctx.errors.append(f"DXY(Yahoo): {str(e)[:40]}")
    
    # æ–¹æ¡ˆ2: ä¸œæ–¹è´¢å¯Œå…¨çƒæŒ‡æ•°ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„æ¥å£ï¼‰
    try:
        import akshare as ak
        df = None
        # å°è¯•ä¸åŒçš„æ¥å£
        for method_name in ['index_global_em', 'tool_trade_date_hist_sina']:
            try:
                if hasattr(ak, method_name):
                    if method_name == 'index_global_em':
                        df = ak.index_global_em()
                    break
            except:
                continue
        
        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»å¤–æ±‡æ•°æ®ä¸­è·å–
        if df is None or df.empty:
            try:
                fx_df = ak.forex_spot_em()
                if fx_df is not None and not fx_df.empty:
                    dxy_row = fx_df[fx_df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
                    if not dxy_row.empty:
                        dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                        if 90 <= dxy_val <= 115:
                            ctx.global_fx["dxy"] = dxy_val
                            ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                            return True
            except:
                pass
        
        if df is not None and not df.empty:
            dxy_row = df[df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
            if not dxy_row.empty:
                dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                if 90 <= dxy_val <= 115:
                    ctx.global_fx["dxy"] = dxy_val
                    ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                    return True
    except Exception as e:
        ctx.errors.append(f"DXY(ä¸œæ–¹è´¢å¯Œ): {str(e)[:40]}")
    
    # ä¸ä½¿ç”¨FREDè´¸æ˜“åŠ æƒæŒ‡æ•°ï¼Œå› ä¸ºèŒƒå›´ä¸åŒä¼šè¯¯å¯¼ç”¨æˆ·
    ctx.errors.append("DXY: ICEç¾å…ƒæŒ‡æ•°è·å–å¤±è´¥")
    ctx.global_fx["dxy"] = None  # æ˜¾å¼è®¾ç½® None
    return False


def fetch_global_fx(ctx: DataContext) -> str:
    """è·å–å…¨çƒå¤–æ±‡æ•°æ®ï¼ˆåŒ…æ‹¬DXYï¼‰"""
    try:
        import akshare as ak
        
        fx_df = None
        for attempt in range(3):
            try:
                fx_df = ak.forex_spot_em()
                if fx_df is not None and not fx_df.empty:
                    break
            except:
                if attempt < 2:
                    time.sleep(2 ** attempt)
        
        found = []
        
        if fx_df is not None and not fx_df.empty:
            pairs = {
                "EURUSD": "eurusd",
                "USDJPY": "usdjpy", 
                "GBPUSD": "gbpusd",
                "AUDUSD": "audusd",
                "USDCAD": "usdcad",
                "USDCHF": "usdchf"
            }
            
            for code, key in pairs.items():
                try:
                    row = fx_df[fx_df['ä»£ç '].str.contains(code, case=False, na=False)]
                    if not row.empty:
                        ctx.global_fx[key] = float(row['æœ€æ–°ä»·'].iloc[0])
                        found.append(code)
                except:
                    pass
            
            # å°è¯•ä»ä¸œæ–¹è´¢å¯Œè·å–DXY
            try:
                dxy_row = fx_df[fx_df['åç§°'].str.contains('ç¾å…ƒæŒ‡æ•°', na=False)]
                if not dxy_row.empty:
                    dxy_val = round(float(dxy_row['æœ€æ–°ä»·'].iloc[0]), 2)
                    if 80 <= dxy_val <= 120:
                        ctx.global_fx["dxy"] = dxy_val
                        ctx.data_sources["dxy"] = "ä¸œæ–¹è´¢å¯Œ"
                        found.append("DXY")
            except:
                pass
        
        # å¦‚æœDXYè¿˜æ²¡è·å–åˆ°ï¼Œä½¿ç”¨ç›´æ¥API
        if "dxy" not in ctx.global_fx:
            if fetch_dxy_direct(ctx):
                found.append("DXY")
        
        if "dxy" not in ctx.global_fx:
            ctx.errors.append("DXY: æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥")
            ctx.global_fx["dxy"] = None  # æ˜¾å¼è®¾ç½® None
        
        return f"âœ… å…¨çƒå¤–æ±‡: {', '.join(found)}" if found else "âš ï¸ å…¨çƒå¤–æ±‡æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        # å³ä½¿AKShareå¤±è´¥ï¼Œä¹Ÿå°è¯•è·å–DXY
        if fetch_dxy_direct(ctx):
            return f"âœ… DXY: {ctx.global_fx.get('dxy')}"
        ctx.errors.append(f"å…¨çƒå¤–æ±‡: {str(e)[:80]}")
        return "âŒ å…¨çƒå¤–æ±‡è·å–å¤±è´¥"


def fetch_fred_data(ctx: DataContext) -> str:
    """è·å– FRED å®è§‚æ•°æ®"""
    fred_key = os.getenv("FRED_API_KEY")
    if not fred_key:
        ctx.errors.append("FRED_API_KEY æœªé…ç½®")
        return "âš ï¸ FRED æœªé…ç½®"
    
    try:
        from fredapi import Fred
        fred = Fred(api_key=fred_key)
        results = []
        
        try:
            us10y = fred.get_series_latest_release("DGS10")
            if us10y is not None and not us10y.empty:
                ctx.macro["us10y"] = round(float(us10y.iloc[-1]), 2)
                ctx.data_sources["us10y"] = "FRED"
                results.append("10Y")
            else:
                ctx.macro["us10y"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"US10Y: {str(e)[:40]}")
            ctx.macro["us10y"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        try:
            us2y = fred.get_series_latest_release("DGS2")
            if us2y is not None and not us2y.empty:
                ctx.macro["us2y"] = round(float(us2y.iloc[-1]), 2)
                results.append("2Y")
            else:
                ctx.macro["us2y"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"US2Y: {str(e)[:40]}")
            ctx.macro["us2y"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        # è®¡ç®—æ”¶ç›Šç‡æ›²çº¿ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None æ—¶æ‰è®¡ç®—ï¼‰
        if ctx.macro.get("us10y") is not None and ctx.macro.get("us2y") is not None:
            ctx.macro["yield_curve"] = round(ctx.macro["us10y"] - ctx.macro["us2y"], 2)
        
        try:
            vix = fred.get_series_latest_release("VIXCLS")
            if vix is not None and not vix.empty:
                vix_val = round(float(vix.iloc[-1]), 2)
                ctx.macro["vix"] = vix_val
                ctx.data_sources["vix"] = "CBOE/FRED"
                results.append("VIX")
                
                if vix_val < 15:
                    ctx.macro["market_sentiment"] = "ä¹è§‚ï¼ˆä½ææ…Œï¼‰"
                elif vix_val < 20:
                    ctx.macro["market_sentiment"] = "ä¸­æ€§"
                elif vix_val < 30:
                    ctx.macro["market_sentiment"] = "è°¨æ…"
                else:
                    ctx.macro["market_sentiment"] = "ææ…Œ"
            else:
                ctx.macro["vix"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"VIX: {str(e)[:40]}")
            ctx.macro["vix"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        try:
            ffr = fred.get_series_latest_release("FEDFUNDS")
            if ffr is not None and not ffr.empty:
                ctx.macro["fed_rate"] = round(float(ffr.iloc[-1]), 2)
                results.append("FedRate")
            else:
                ctx.macro["fed_rate"] = None  # API è¿”å›ç©ºæ•°æ®ï¼Œæ˜¾å¼è®¾ç½® None
        except Exception as e:
            ctx.errors.append(f"FedRate: {str(e)[:40]}")
            ctx.macro["fed_rate"] = None  # API å¤±è´¥ï¼Œæ˜¾å¼è®¾ç½® None
        
        return f"âœ… FRED: {', '.join(results)}" if results else "âš ï¸ FRED æ•°æ®ç¼ºå¤±"
        
    except Exception as e:
        ctx.errors.append(f"FRED: {str(e)[:80]}")
        return "âŒ FRED è·å–å¤±è´¥"


def fetch_perplexity_news(ctx: DataContext) -> str:
    """ä½¿ç”¨ Perplexity API è·å–å¤–æ±‡ç›¸å…³æ–°é—»
    
    æ–¹æ¡ˆBï¼šåˆ†ä¸¤æ¬¡è°ƒç”¨ï¼Œåˆ†åˆ«æœç´¢è‹±æ–‡å’Œä¸­æ–‡æ¥æº
    """
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        ctx.errors.append("PERPLEXITY_API_KEY æœªé…ç½®")
        return "âš ï¸ Perplexity æœªé…ç½®"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    from datetime import timedelta
    today_date = datetime.now()
    week_ago = today_date - timedelta(days=7)
    today_display = today_date.strftime("%B %d, %Y")
    week_ago_display = week_ago.strftime("%B %d, %Y")
    today_cn = today_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
    week_ago_cn = week_ago.strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    # é…ç½®ä»£ç†
    proxies = None
    socks5_proxy = os.getenv("SOCKS5_PROXY")
    http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
    https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")
    
    if socks5_proxy:
        try:
            import socks
            if not socks5_proxy.startswith("socks5://"):
                socks5_url = f"socks5h://{socks5_proxy}"
            else:
                socks5_url = socks5_proxy.replace("socks5://", "socks5h://")
            proxies = {"http": socks5_url, "https": socks5_url}
        except ImportError:
            ctx.errors.append("éœ€è¦: pip install requests[socks]")
    elif http_proxy or https_proxy:
        proxies = {"http": http_proxy, "https": https_proxy or http_proxy}
    
    # ========== è‹±æ–‡ Prompt ==========
    payload_en = {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": f"""You are an FX market news editor. Search ONLY English sources from {week_ago_display} to {today_display}.

Sources: Bloomberg, Reuters, Financial Times, WSJ, Federal Reserve, ECB, BOJ.

Output format - for each news item:
1. [EN]
TITLE: Headline here [1]
DETAIL: 150-200 word summary with specific data [1]

2. [EN]
TITLE: Next headline [2]
DETAIL: Summary [2]

IMPORTANT: 
- Each news starts with number and [EN] on its own line
- TITLE and DETAIL on separate lines
- Every line must end with citation [1], [2], etc."""
            },
            {
                "role": "user", 
                "content": f"Find 8 important English FX news. Topics: Fed/FOMC, DXY, EUR/USD, USD/JPY, Treasury yields. Use the exact format specified."
            }
        ],
        "max_tokens": 4000,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }
    
    # ========== ä¸­æ–‡ Prompt ==========
    payload_cn = {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯å¤–æ±‡å¸‚åœºæ–°é—»ç¼–è¾‘ã€‚åªæœç´¢ {week_ago_cn} è‡³ {today_cn} çš„ä¸­æ–‡æ¥æºã€‚

æ¥æºï¼šå¤®è¡Œå®˜ç½‘(pbc.gov.cn)ã€å¤–ç®¡å±€(safe.gov.cn)ã€è´¢æ–°ç½‘ã€ç¬¬ä¸€è´¢ç»ã€é‡‘ç®¡å±€(hkma.gov.hk)ã€‚

è¾“å‡ºæ ¼å¼ - æ¯æ¡æ–°é—»ï¼š
1. [CN]
TITLE: æ ‡é¢˜å†…å®¹ [1]
DETAIL: 150-200å­—æ‘˜è¦ï¼ŒåŒ…å«å…·ä½“æ•°æ® [1]

2. [CN]
TITLE: ä¸‹ä¸€æ¡æ ‡é¢˜ [2]
DETAIL: æ‘˜è¦å†…å®¹ [2]

é‡è¦ï¼š
- æ¯æ¡æ–°é—»ä»¥æ•°å­—å’Œ[CN]å¼€å¤´ï¼Œå•ç‹¬ä¸€è¡Œ
- TITLEå’ŒDETAILåˆ†å¼€ä¸¤è¡Œ
- æ¯è¡Œç»“å°¾å¿…é¡»æœ‰å¼•ç”¨æ ‡è®°[1], [2]ç­‰"""
            },
            {
                "role": "user", 
                "content": f"æœç´¢7æ¡é‡è¦ä¸­æ–‡å¤–æ±‡æ–°é—»ã€‚ä¸»é¢˜ï¼šäººæ°‘å¸ä¸­é—´ä»·ã€USD/CNYã€å¤®è¡Œæ”¿ç­–ã€æ¸¯å…ƒã€é‡‘ç®¡å±€ã€‚ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºã€‚"
            }
        ],
        "max_tokens": 3500,
        "temperature": 0.1,
        "return_citations": True,
        "search_recency_filter": "week"
    }
    
    # ========== è§£æå‡½æ•° ==========
    def parse_response(content, citations, lang_tag):
        """è§£æ Perplexity è¿”å›å†…å®¹"""
        # æå–æœ‰æ•ˆ URLs
        valid_urls = []
        for c in citations:
            if isinstance(c, str) and c.startswith('http'):
                valid_urls.append(c)
            elif isinstance(c, dict):
                url = c.get('url') or c.get('link')
                if url and url.startswith('http'):
                    valid_urls.append(url)
        
        lines = content.strip().split('\n')
        news_items = []
        current = {'title': '', 'detail': '', 'refs': []}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹æ–°é—»å¼€å§‹: "1. [EN]" æˆ– "1. [CN]" æˆ– "1.[EN]" ç­‰
            start_match = re.match(r'^(\d+)[\.\)]\s*\[(EN|CN)\]\s*$', line, re.IGNORECASE)
            if start_match:
                # ä¿å­˜ä¸Šä¸€æ¡
                if current['title']:
                    news_items.append(current.copy())
                current = {'title': '', 'detail': '', 'refs': []}
                continue
            
            # æ£€æµ‹ TITLE è¡Œ
            if line.upper().startswith('TITLE:'):
                # æå–å¼•ç”¨æ ‡è®°
                refs = re.findall(r'\[(\d+)\]', line)
                current['refs'].extend([int(r)-1 for r in refs if r.isdigit()])
                # æ¸…é™¤å¼•ç”¨æ ‡è®°åå­˜å‚¨
                current['title'] = re.sub(r'\s*\[\d+\]\s*', ' ', line[6:]).strip()
                continue
            
            # æ£€æµ‹ DETAIL è¡Œ
            if line.upper().startswith('DETAIL:'):
                refs = re.findall(r'\[(\d+)\]', line)
                current['refs'].extend([int(r)-1 for r in refs if r.isdigit()])
                current['detail'] = re.sub(r'\s*\[\d+\]\s*', ' ', line[7:]).strip()
                continue
            
            # ç´¯ç§¯ DETAILï¼ˆå¤šè¡Œæƒ…å†µï¼‰
            if current['title'] and not re.match(r'^\d+[\.\)]\s*\[', line):
                refs = re.findall(r'\[(\d+)\]', line)
                current['refs'].extend([int(r)-1 for r in refs if r.isdigit()])
                clean = re.sub(r'\s*\[\d+\]\s*', ' ', line).strip()
                if clean and not clean.startswith('#'):
                    current['detail'] += ' ' + clean
        
        # ä¿å­˜æœ€åä¸€æ¡
        if current['title']:
            news_items.append(current.copy())
        
        # æ„å»ºç»“æœ
        results = []
        for item in news_items:
            title = f"[{lang_tag}] TITLE: {item['title']}"
            detail = item['detail'] if item['detail'] else item['title']
            # åˆ†é… URLs
            urls = []
            for ref_idx in set(item['refs']):
                if 0 <= ref_idx < len(valid_urls):
                    urls.append(valid_urls[ref_idx])
            results.append((title, detail, urls))
        
        return results, len(valid_urls)
    
    # ========== æ‰§è¡Œ API è°ƒç”¨ ==========
    all_news = []
    total_urls = 0
    en_count = 0
    cn_count = 0
    
    try:
        session = requests.Session()
        
        # è°ƒç”¨è‹±æ–‡ API
        try:
            resp_en = session.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers, json=payload_en,
                timeout=TIMEOUT_CONFIG["perplexity"], verify=False, proxies=proxies  # verify=False ä¸ºå…¼å®¹ä»£ç†ç¯å¢ƒ
            )
            if resp_en.status_code == 200:
                result = resp_en.json()
                content = result['choices'][0]['message']['content']
                citations = result.get('citations', [])
                if not citations:
                    citations = result['choices'][0].get('message', {}).get('citations', [])
                news_en, urls_en = parse_response(content, citations, 'EN')
                all_news.extend(news_en)
                total_urls += urls_en
                en_count = len(news_en)
            else:
                ctx.errors.append(f"Perplexity EN: {resp_en.status_code}")
        except Exception as e:
            ctx.errors.append(f"Perplexity EN: {str(e)[:50]}")
        
        # è°ƒç”¨ä¸­æ–‡ API
        try:
            resp_cn = session.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers, json=payload_cn,
                timeout=TIMEOUT_CONFIG["perplexity"], verify=False, proxies=proxies  # verify=False ä¸ºå…¼å®¹ä»£ç†ç¯å¢ƒ
            )
            if resp_cn.status_code == 200:
                result = resp_cn.json()
                content = result['choices'][0]['message']['content']
                citations = result.get('citations', [])
                if not citations:
                    citations = result['choices'][0].get('message', {}).get('citations', [])
                news_cn, urls_cn = parse_response(content, citations, 'CN')
                all_news.extend(news_cn)
                total_urls += urls_cn
                cn_count = len(news_cn)
            else:
                ctx.errors.append(f"Perplexity CN: {resp_cn.status_code}")
        except Exception as e:
            ctx.errors.append(f"Perplexity CN: {str(e)[:50]}")
        
        # å­˜å‚¨ç»“æœ
        news_with_urls = 0
        for title, detail, urls in all_news:
            ctx.news.append(title)
            ctx.news_detail.append(detail)
            ctx.news_sources.append(urls)
            if urls:
                news_with_urls += 1
        
        ctx.data_sources["news"] = "Perplexity(EN+CN)"
        ctx.data_sources["news_valid_urls"] = total_urls
        
        return f"âœ… æ–°é—»: {len(all_news)} æ¡ (EN:{en_count} + CN:{cn_count}, æœ‰é“¾æ¥: {news_with_urls}/{len(all_news)})"
        
    except Exception as e:
        ctx.errors.append(f"Perplexity: {str(e)[:60]}")
        return f"âš ï¸ Perplexity: {str(e)[:40]}"


def calculate_metrics(ctx: DataContext) -> str:
    results = []
    
    # è®¡ç®—æ¸¯ç¾åˆ©å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None ä¸”è¿˜æœªè®¡ç®—æ—¶ï¼‰
    if (ctx.hkd.get("hibor_overnight") is not None and 
        ctx.macro.get("fed_rate") is not None and 
        "hkd_usd_spread" not in ctx.hkd):
        ctx.hkd["hkd_usd_spread"] = round(ctx.hkd["hibor_overnight"] - ctx.macro["fed_rate"], 2)
        results.append("æ¸¯ç¾åˆ©å·®")
    
    # è®¡ç®— CNY ä»·å·®ï¼ˆåªæœ‰åœ¨ä¸¤ä¸ªå€¼éƒ½ä¸ä¸º None ä¸”è¿˜æœªè®¡ç®—æ—¶ï¼‰
    if (ctx.cny.get("usdcny_mid") is not None and 
        ctx.cny.get("usdcnh_spot") is not None and 
        "cny_spread" not in ctx.cny):
        ctx.cny["cny_spread"] = round(ctx.cny["usdcnh_spot"] - ctx.cny["usdcny_mid"], 4)
        results.append("CNYä»·å·®")
    
    return f"âœ… è®¡ç®—å®Œæˆ"


def retrieve_all_data(progress_callback: Optional[ProgressCallback] = None) -> DataContext:
    ctx = DataContext()
    
    steps = [
        ("FRED å®è§‚æ•°æ®", lambda: fetch_fred_data(ctx)),
        ("äººæ°‘å¸æ•°æ®", lambda: fetch_cny_data(ctx)),
        ("æ¸¯å…ƒæ•°æ®", lambda: fetch_hkd_data(ctx)),
        ("å…¨çƒå¤–æ±‡", lambda: fetch_global_fx(ctx)),
        ("Perplexity æ–°é—»", lambda: fetch_perplexity_news(ctx)),
        ("è®¡ç®—è¡ç”ŸæŒ‡æ ‡", lambda: calculate_metrics(ctx)),
    ]
    
    total = len(steps)
    
    for i, (name, func) in enumerate(steps):
        if progress_callback:
            progress_callback(i, total, f"ğŸ“Š {name}...")
        
        try:
            result = func()
            if progress_callback:
                progress_callback(i + 1, total, result)
        except Exception as e:
            ctx.errors.append(f"{name}: {str(e)[:50]}")
            if progress_callback:
                progress_callback(i + 1, total, f"âŒ {name} å¤±è´¥")
    
    return ctx


if __name__ == "__main__":
    def print_progress(step, total, msg):
        print(f"[{step}/{total}] {msg}")
    
    ctx = retrieve_all_data(print_progress)
    print("\n" + "="*60)
    print(ctx.to_json())
